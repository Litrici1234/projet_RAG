{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conception d'un agent conversationnel sp√©cialis√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper le site Ecofin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url √† scraper \n",
    "url=\"https://www.agenceecofin.com/a-la-une/recherche-article?filterTitle=&submit.x=0&submit.y=0&filterTousLesFils=Tous&filterCategories=Sous-rubrique&filterDateFrom=&filterDateTo=&option=com_dmk2articlesfilter&view=articles&filterFrench=French&Itemid=269&userSearch=1&layout=#dmk2articlesfilter_results\"\n",
    "root=\"https://www.agenceecofin.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultation des fichier roboot txt des sites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_robots_txt(url):\n",
    "        \"\"\"R√©cup√®re le contenu du fichier robots.txt\"\"\"\n",
    "        try:\n",
    "            parsed_url = urlparse(url)\n",
    "            name = parsed_url.netloc\n",
    "            # Construire l'URL du robots.txt\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                url = 'https://' + url\n",
    "            robots_url = urljoin(url, '/robots.txt')\n",
    "            \n",
    "            # Faire la requ√™te\n",
    "            response = requests.get(robots_url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(f\"robot_{name}.txt\", \"w\") as file:\n",
    "                    file.write(response.text)\n",
    "                print(\"fichier cr√©e\")\n",
    "            else:\n",
    "                print(f\"Erreur {response.status_code} pour {robots_url}\")\n",
    "               \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur pour {url}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fichier cr√©e\n"
     ]
    }
   ],
   "source": [
    "get_robots_txt(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper le site \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Configuration du navigateur Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Ex√©cuter sans interface graphique\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# üîπ URL de d√©part\n",
    "start_url = \"https://www.agenceecofin.com/a-la-une/recherche-article?filterTitle=&submit.x=0&submit.y=0&filterTousLesFils=Tous&filterCategories=Sous-rubrique&filterDateFrom=&filterDateTo=&option=com_dmk2articlesfilter&view=articles&filterFrench=French&Itemid=269&userSearch=1&layout=#dmk2articlesfilter_results\"  # Remplace avec l'URL du site\n",
    "driver.get(start_url)\n",
    "time.sleep(8)  # Attendre le chargement\n",
    "\n",
    "# üîπ Trouver tous les liens des articles\n",
    "article_links = []\n",
    "i=2\n",
    "for link in driver.find_elements(By.CSS_SELECTOR, \"#dmk2articlesfilter_results > table:nth-child(i) > tbody > tr > td.tsw > h3\"):  # MODIFIER ICI\n",
    "    # #dmk2articlesfilter_results > table:nth-child(2) > tbody > tr > td.tsw > h3\n",
    "    # #dmk2articlesfilter_results > table:nth-child(3) > tbody > tr > td.tsw > h3\n",
    "    # #dmk2articlesfilter_results > table:nth-child(8) > tbody > tr > td.tsw > h3\n",
    "    href = link.get_attribute(\"href\")\n",
    "    if href and \"article\" in href:  # V√©rifier que c'est un lien valide\n",
    "        article_links.append(href)\n",
    "    i+=1\n",
    "\n",
    "print(f\"üîç Trouv√© {len(article_links)} articles √† scraper...\")\n",
    "\n",
    "articles = []\n",
    "\n",
    "# üîπ Parcourir chaque article et scraper les infos\n",
    "for url in article_links:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Attendre le chargement de la page\n",
    "\n",
    "        # ‚úÖ MODIFIER ICI avec les bons s√©lecteurs CSS/XPath pour ton site\n",
    "        title = driver.find_element(By.CSS_SELECTOR, \"#k2Container > div.itemHeader > h1\").text  # Exemple\n",
    "        \n",
    "        date = driver.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")  # Exemple\n",
    "        author = driver.find_element(By.CSS_SELECTOR, \".author-name\").text  # Exemple\n",
    "        category = driver.find_element(By.CSS_SELECTOR, \".category-name\").text  # Exemple\n",
    "        content = \"\\n\".join([p.text for p in driver.find_elements(By.CSS_SELECTOR, \"div.article-content p\")])  # Exemple\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"date\": date,\n",
    "            \"author\": author,\n",
    "            \"category\": category,\n",
    "            \"content\": content,\n",
    "            \"url\": url\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du scraping de {url}: {e}\")\n",
    "\n",
    "# üîπ Sauvegarde dans un fichier JSON\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# üîπ Fermer le navigateur\n",
    "driver.quit()\n",
    "\n",
    "print(\"‚úÖ Scraping termin√© avec succ√®s ! üöÄ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialisation du mod√®le d'embedding pour la recherche s√©mantique\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = \"article_spider\"\n",
    "    start_urls = [\"https://example.com\"]  # Remplacez par l'URL du site √† scraper\n",
    "\n",
    "    def parse(self, response):\n",
    "        for article in response.css(\"article\"):  # Adapter le s√©lecteur selon le site\n",
    "            title = article.css(\"h1::text, h2::text\").get()\n",
    "            summary = article.css(\"p::text\").get()\n",
    "            date = article.css(\"time::attr(datetime)\").get()\n",
    "            author = article.css(\".author::text\").get()\n",
    "            category = article.css(\".category::text\").get()\n",
    "            content = \"\\n\".join(article.css(\"p::text\").getall())\n",
    "            url = response.url\n",
    "            \n",
    "            # G√©n√©ration de l'embedding pour la recherche s√©mantique\n",
    "            embedding = model.encode(content).tolist()\n",
    "            \n",
    "            article_data = {\n",
    "                \"title\": title,\n",
    "                \"summary\": summary,\n",
    "                \"date\": date,\n",
    "                \"author\": author,\n",
    "                \"category\": category,\n",
    "                \"content\": content,\n",
    "                \"url\": url,\n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "            \n",
    "            yield article_data\n",
    "\n",
    "# Commande pour ex√©cuter le script et sauvegarder les donn√©es\n",
    "# scrapy runspider scraper.py -o articles.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec selenuim\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Initialisation du mod√®le d'embedding\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Configuration du navigateur\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Mode sans interface graphique\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# URL du site √† scraper\n",
    "start_url = \"https://example.com\"\n",
    "driver.get(start_url)\n",
    "\n",
    "# Attendre le chargement de la page\n",
    "time.sleep(3)\n",
    "\n",
    "articles = []\n",
    "\n",
    "# Extraction des articles\n",
    "article_elements = driver.find_elements(By.TAG_NAME, \"article\")  # Adapter le s√©lecteur selon le site\n",
    "for article in article_elements:\n",
    "    try:\n",
    "        title = article.find_element(By.TAG_NAME, \"h1\").text or article.find_element(By.TAG_NAME, \"h2\").text\n",
    "        summary = article.find_element(By.TAG_NAME, \"p\").text\n",
    "        date = article.find_element(By.TAG_NAME, \"time\").get_attribute(\"datetime\")\n",
    "        author = article.find_element(By.CLASS_NAME, \"author\").text\n",
    "        category = article.find_element(By.CLASS_NAME, \"category\").text\n",
    "        content = \"\\n\".join([p.text for p in article.find_elements(By.TAG_NAME, \"p\")])\n",
    "        url = driver.current_url\n",
    "        \n",
    "        # G√©n√©ration de l'embedding\n",
    "        embedding = model.encode(content).tolist()\n",
    "        \n",
    "        article_data = {\n",
    "            \"title\": title,\n",
    "            \"summary\": summary,\n",
    "            \"date\": date,\n",
    "            \"author\": author,\n",
    "            \"category\": category,\n",
    "            \"content\": content,\n",
    "            \"url\": url,\n",
    "            \"embedding\": embedding\n",
    "        }\n",
    "        \n",
    "        articles.append(article_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction d'un article: {e}\")\n",
    "\n",
    "# Sauvegarde des donn√©es en JSON\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Fermeture du navigateur\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
